<head>
    <title>Math</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="main.css">
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
</head>

<body>
    <h1>Math</h1>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/gENVB6tjq_M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

    <ul>
    	<li><a href="math.html#prob">Probability and Statistics</a></li>
    	<li><a href="math.html#nlp">Natural Language Processing</a></li>
    	<li><a href="math.html#lnalg">Linear Algebra</a></li>
    	<li><a href="math.html#pca">Principle Component Analysis</a></li>
    </ul>

    <article id="prob">
    <h2>Probability and Statistics</h2>
	<p><b>Sample space</b>: a space of events that are assigned probabilities, where events are binary, multi-valued, or continuous; but always mutually exclusive</p>
	<p class="examples">Examples
	<ul>
		<li>Coin flip: {head, tail}</li>
		<li>Die roll: {1, 2, 3, 4, 5, 6}</li>
	</ul></p>
	<p><b>Random variable</b>: a variable, \(x\), whose domain is the sample space and whose value is somewhat uncertain
	<ul>
		<li>\(x\) = coin flip outcome</li>
		<li>\(x\) = tomorrow's temperature</li>
	</ul></p>
	
	<h3>The Axioms of Probability</h3>
	<ol>
		<li>For event \(A, P(A) &isin; [0,1]\)</li>
		<li>For sample space \(S\), \(P(S) = 1\); and \(P(true) = 1\), \(P(false) = 0\)</li>
		<li>If events \(A_1, A_2, A_3, \cdots\) are disjoint (mutually exclusive), then \(P(A_1 \cup A_2 \cup A_3 \cdots)=P(A_1)+P(A_2)+P(A_3)+\cdots\)<br>
		It follows that for disjoint events \(A\) and \(B\)<br>
		\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</li>
	</ol>
	
	<p><b>Joint probability</b>: the probability of two events occurring together at the same point in time.</p>
	$$P(A, B) = P(A \cap B) = P(A)*P(B)$$
	
	<p><b>Marginal probability</b>: the probability of a single event occurring unconditioned on any other events</p>

	<p><b>Conditional probability</b>: the probability of an event \(B\) given that event \(A\) has already occurred, and that these events are dependent</p>
	$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$
	For conditional probability on some other events \(C\)
	$$P(A| B,C) = \frac{P(A,B |C)}{P(B|C)}$$

	<p><b>Chain rule</b>: derived from the definition of conditional probability</p>
	$$P(A, B) = P(B)*P(A|B) = P(A)*P(B|A)$$
	$$P(A_1,A_2,\cdots,A_n) = P(A_1)*P(A_2|A_1)*P(A_3|A_1,A_2)*\cdots*P(A_n|A_1,A_2 \cdots A_{n-1})$$

	<p><b>Bayes' Rule</b></p>
	$$P(A|B) = \frac{P(A,B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$$
	<ul>
		<li><b>Prior \(P(A)\)</b>: probability before any evidence</li>
		<li><b>Likelihood \(P(B|A)\)</b>: assuming \(A\), how likely is the evidence</li>
		<li><b>Posterior \(P(A|B)\)</b>: conditional probability after knowing evidence</li>
		<li><b>Inference</b>: deriving unknown probability from known ones</li>
	</ul>

	<p><b>Identities for Independent Events</b></p>
	<ul>
		<li>\(P(A,B) = P(A)*P(B)\)</li>
		<li>\(P(A|B) = P(A)\)</li>
		<li>\(P(B|A) = P(B)\)</li>
	</ul>

	<p><b>Conditional independence</b>: two events \(A\) and \(B\) are conditionally independent given an event \(C\)</p>
	$$P(A| B,C) = \frac{P(A \cap B |C)}{P(B|C)} = \frac{P(A|C)P(B|C)}{P(B|C)} = P(A|C)$$
	</article>

	<article id="nlp">
    <h2>Natural Language Processing</h2>
    <p><b>Word token</b>: occurrences of a word</p>
    <p><b>Word type</b>: unique word</p>
    <p><b>Vocabulary</b>: list of the word types</p>
    <p><b>Zipf's Law</b>: given a large sample of words used, the frequency of any word is inversely proportional to its rank on the frequency table</p>
    <p><b>Corpus</b> (<i>pl.</i> corpora): a large, representative collection of text</p>
    <p><b>Multinomial distribution</b> for a \(k\)-sided die with probability vector \(\theta\), \(N\) throws, outcomes counts \(n_1, \cdots,n_k\)</p>
    $$P(n_1,\cdots ,n_k|\theta) = \binom{N}{n_1 \cdots n_k} = \prod_{i=1}^{k} \theta_i^{n_i}$$
    <p><b>The Maximum Likelihood Estimate (MLE)</b></p>
    <p><b>The Maximum A Posterior (MAP) Estimate</b></p>
    <p><b>Language modeling</b>: tries to capture the notion that some text is more likely than others by estimating the probability \(P(s)\) of any text \(s\)</p>
    <p><b>Unigram Language Model</b>: makes a strong <i>independence assumption</i> that words are generated independently from a multinomial distribution \(\theta\) (of dimension \(V\) = size of the vocabulary)</p>
    <p><b>N-gram Language Models</b></p>
    <p><b>Smoothing</b></p>
    add-1 smoothing
    $$\hat{\theta_w} = \frac{c_w + 1}{|C| + V'}$$
    add-&isin; smoothing
    $$\hat{\theta_w} = \frac{c_w + \in}{|C| + V\in'}$$
	</article>

	<article id="lnalg">
    <h2>Linear Algebra</h2>
	<b>Scalar</b>: a number, has magnitude (1 &times; 1)<br>
	<b>Vector</b>: a list of numbers, has magnitude and direction (default column vector n &times; 1)<br>
	<b>Matrix</b>: an array of numbers (m &times; n)
	<h3>Matrix Basics</h3>
	<p><b>Transpose</b>: \((A^T)_{ij} = A_{ji}\) and \((A+B)^{\top} = A^{\top}+B^{\top}\) and \((AB)^{\top} = B^{\top}A^{\top}\). If \(A = A^{\top}\) then matrix \(A\) is <b>symmetric</b></p>
	$$\begin{bmatrix} 1 & 2 & 3 \\
	4 & 5 & 6 \end{bmatrix}^\top = 
	\begin{bmatrix} 1 & 4 \\
	2 & 5 \\
	3 & 6 \end{bmatrix}$$
	<p><b>Multiplication</b>: if matrix \(A\) is \(m \times n\) and matrix \(B\) is \(n \times p\), then \(AB = C\) and \(C\) is \(m \times p\) and \(C_{ij} = \sum\limits_{k=1}^m A_{ik}B_{kj}\) and generally \(AB \neq BA\)</p>
	$$\begin{bmatrix} 2 & 8 & 3 \\
	5 & 4 & 1 \end{bmatrix}
	\begin{bmatrix} 4 & 1 \\
	6 & 3 \\
	2 & 4 \end{bmatrix} = 
	\begin{bmatrix} 2(4)+8(6)+3(2) & 2(1)+8(3)+3(4) \\
	5(4)+4(6)+1(2) & 5(1)+4(3)+1(4) \end{bmatrix} = 
	\begin{bmatrix} 62 & 38 \\
	46 & 21 \end{bmatrix}$$
	<h3>Square Matrices</h3>
	<p><b>Diagonal Matrix</b>: nonzero entries along diagonal, zeroes elsewhere (example is 3 &times; 3 but can be any size)</p>
	$$\begin{bmatrix} x_1 & 0 & 0 \\
	0 & x_2 & 0 \\
	0 & 0 & x_3 \end{bmatrix}$$
	<p><b>Determinant of a Matrix</b>: scaling factor of the linear transformation described by the matrix, written as \(det(A)\) or \(|A|\) and matrix \(A\) is invertible iff \(|A| \neq 0\). If \(|A| = 0\) then matrix \(A\) is <b>singular</b> and therefore <b>linearly dependent</b></p>
	$$|A| = \begin{vmatrix} a & b \\
	c & d \end{vmatrix} = ad - bc, |A^{-1}| = \frac{1}{|A|}$$
	<p><b>Inverse of a Matrix</b>: matrix \(A^{-1}\) such that \(AA^{-1} = I\), where \(I\) is the identity matrix. \((AB)^{-1} = B^{-1}A^{-1}\) and \((A^{\top})^{-1} = (A^{-1})^{\top}\)</p>
	$$A = \begin{bmatrix} a & b \\
	c & d \end{bmatrix}, A^{-1} = \frac{1}{det(A)} adj(A) = 
	\frac{1}{ad-bc} \begin{bmatrix} d & -b \\
	-c & a \end{bmatrix}$$
	<p><b>Identity Matrix</b>: diagonal matrix whose nonzero entries are all 1. \(AI = IA = A\) and \(AA^{-1} = A^{-1}A = I\)</p>
	<p><b>Trace of a Matrix</b>: sum of elements along the diagonal, for \(n \times n\) matrix \(A\), \(tr(A) = \sum\limits_{i=1}^n a_{ii}\)</p>
	$$A = \begin{bmatrix}\def\b{\color{blue}} \b 1 & 2 & 3 \\
	4 & \b 5 & 6 \\
	7 & 8 & \b 9 \end{bmatrix}, tr(A) = 1+5+9 = 15$$
	<p><b>Eigenspaces</b></p>
	A \(m \times m\) matrix \(A\) has \(m\) <b>eigenvalues</b> &lambda; and \(m\) <b>eigenvectors</b> \(x_i\) such that \(Ax_1 = \lambda x_i\). <a href="https://piazza.com/class/jlmhmm29tn87jk?cid=366">It is not necessary to know how to compute eigen decomposition</a>, but the general form used to do so is \(det(A- \lambda I) = 0\).
	</article>

	<article id="calc">
	<h2>Calculus</h2>
	</article>
	<p><b>Derivative</b>: slope of a tangent line</p>
	$$f'(x) = \frac{df}{dx} = \lim_{\delta \to 0} \frac{f(x+ \delta) - f(x)}{\delta}$$
	<p><b>Second Derivative</b>: curvature</p>
	$$f''(x) = \frac{d^2 f}{dx^2} = {df'}{dx}$$
	<p><b>Chain Rule</b></p>
	$$\frac{df(y)}{dx} = \frac{df(y)}{dy} \frac{y}{dx}$$

	<article id="pca">
    	<h2>Prinicipal Component Analysis</h2>
    	<p>PCA is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set. This procedure transforms a number of possibily correlated variables into a smaller number of uncorrelated variables called <b>principal components</b>. PCA is traditionally performed on a <b>square, symmetric</b> matrix.</p>
	<ol>
		<li>Let \(x_1, \dots, x_n \in \mathbb{R}^D\) where \(\mathbb{R}^D\) is the set of all real numbers in the \(D^{th}\)-dimension. PCA is to be performed on a set of <i>centered</i> points, i.e., \(\sum_i x_i = 0\). Center the data by computing the sample mean, \(\mu\) and subtracting it from each data point.</li>
		$$\mu = frac{1}{n} \sum_{i=1}^{n} x_i, x_i = x_i - \mu$$
		<li>Compute the sample covariance matrix \(S\) using a matrix of the centered data \(X\) that will be \(n \times D\)</li>
		$$S = frac{1}{n-1} X^{\intercal}X$$
		<li>Perform eigen decomposition. <b><a href="https://piazza.com/class/jlmhmm29tn87jk?cid=366">You do not need to know how to do this!</a></b> Use <a href="http://www.bluebit.gr/matrix-calculator/">this site</a> to calculate eigenvalues \(\lambda_1, \dots, \lambda_D\) and corresponding eigenvectors \(u_1, \dots, u_D\)</li>
		$$S = U \Lambda U^{\intercal}, U = \begin{bmatrix} u_1 & \dots & u_D \end{bmatrix}, \Lambda = \begin{bmatrix} \lambda_1 & 0 & 0 \\ \vdots & \ddots & 0 \\ 0 & \dots & \lambda_D \end{bmatrix}$$
		<li>Compute the first principal component \(v_1\) of dimension \(d\) where \(d < D\) i'll finish this later lol just multiply ur eigenvectors by x</li>
	</ol>
		
	</article>
    
</body>
